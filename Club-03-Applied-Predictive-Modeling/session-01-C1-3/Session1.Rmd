---
title: "Applied Predictive Modeling"
subtitle: "Chapters 1~3 "
author: "John Peach"
institute: "OCRUG Book Club"
date: "2020-05-27"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, intro.css]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introductions
.large[
* Introduce ourselves
* What we are trying to do
* Expectations
]

---

# Chapter 1
.large[
* Prediction vs. Interpretation
]

---

# Chapter 2
## Model 1
.medium[
* One predictor vs One response
* Using 2010 to predict model year 2011 Fuel Economy
    * Not sub-sampling the data
* Performance measurement: RMSE
* Use resampling to evaluate model performance 
]
```{r, echo=FALSE, out.height='300px', fig.align='center'}
knitr::include_graphics('chapter2chart1.png')
```
---

# Chapter 2
## Model 2
.medium[
* Add a squared term for engine displacement
* Performed poorly on the extreme values 
]
```{r, echo=FALSE, out.height='300px', fig.align='center'}
knitr::include_graphics('chapter2chart2.png')
```

---
# Chapter 2
## Model 3
.medium[
* Multivariate adaptive regression spline (MARS) model
* Using hinge function for prediction
* Tuning parameter: number of segments
    * Use CV to tune
]
```{r, echo=FALSE, out.height='300px', fig.align='center'}
knitr::include_graphics('1.png')
```
---
# Chapter 2
## Themes
.large[
* Data splitting
* Predictor data / feature selection
* Estimating Performance (RMSE, etc.)
* Models evaluation: "No Free Lunch"
* Model selection
]

---

# Chapter 3
## Data Pre-processing 
###  Overview
.large[
* Definition: addition, deletion, or transformation of training data set
* Supervised data processing vs unsupervised
* Feature engineering
]

---
# Chapter 3
## Data Pre-processing 
### Data transformations for individual predictors
.large[
* centering and scaling
* distribution skewness
```{r, echo=FALSE, out.height='150px', fig.align='center'}
knitr::include_graphics('chapter3chart1.png')
```
* Box and Cox transformation
```{r, echo=FALSE, out.height='100px', fig.align='center'}
knitr::include_graphics('chapter3chart2.png')
```
]
---
# Chapter 3
## Data Pre-processing 
### Data transformations for multiple predictors
.large[
* outliers
* spatial sign
```{r, echo=FALSE, out.height='100px', fig.align='center'}
knitr::include_graphics('chapter3chart3.png')
```

```{r, echo=FALSE, out.height='200px', fig.align='center'}
knitr::include_graphics('chapter3chart4.png')
```
]
---
# Chapter 3
## Data Pre-processing 
### Data reduction and feature extraction
.medium[
* Seeks to capture a majority of the information in the original variables
* PCA

```{r, echo=FALSE, out.height='50px', fig.align='center'}
knitr::include_graphics('chapter3chart5.png')
```
```{r, echo=FALSE, out.height='300px', fig.align='center'}
knitr::include_graphics('chapter3chart6.png')
```
]
---
# Chapter 3
## Data Pre-processing 
### Caveats 
.medium[
* Cells are not well separated
* Loadings near zero indicate that the feature is not important in a given component
* There are clustering of the loadings
]
```{r, echo=FALSE, out.height='250px', fig.align='center'}
knitr::include_graphics('chapter3chart9.png') 
```

---
# Chapter 3
## Data Pre-processing 
### Dealing with Missing Values
.medium[
* Structurally missing
* Missing at random
* Missing completely at random
* Censored
]

---
# Chapter 3
## Data Pre-processing 
### Dealing with Missing Values
.medium[
* Small literature on missing data in prediction
* KNN is popular imputation method
]
```{r, echo=FALSE, out.height='300px', fig.align='center'}
knitr::include_graphics('chapter3chart10.png') 
```

---
# Chapter 3
## Data Pre-processing 
### Removing Predictors
.large[
* Decrease computational cost
* Highly correlated can cause problems
* Zero variance predictor
* Categorical variable inflation (dummy vars)
]

---

# Chapter 3
## Data Pre-processing 
### Between Predictor Correlations
.medium[
* Collinearity / multicollinearity
* Variance inflation factor (VIF)
* Remove correlated values
    1. Calc correlation matrix
    1. Let A, B be highest correlation
    1. Avg correlations across all pred for A, B
    1. Remove the pred with the highest avg
    1. Repeat until no pairs meet the threshold
]
---

# Chapter 3
## Data Pre-processing 
### Adding Predictors
.large[
```{r, echo=FALSE, out.height='300px', fig.align='center'}
knitr::include_graphics('chapter3chart11.png') 
```
]
---

# Chapter 3
## Data Pre-processing 
### Binning Predictors
.large[
* Yeah, just do not do this
* Some algos (trees) do it and that is fine
]
---

# Problems
.large[
* 3.1
* 3.2
* 3.3
]
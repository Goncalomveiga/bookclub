---
title: "Mastering Spark with R"
subtitle: 'Chapters 1-3'
author: "John Peach"
institute: "Orange County R Users Group"
date: "2021-02-01"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts, intro.css]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      
---
<style>
pre {
    display: block;
    font-family: monospace;
    white-space: pre;
    margin: 1em 0px;
    margin-top: 0em;
    margin-right: 0px;
    margin-bottom: 0em;
    margin-left: 0px;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(comment = "")
```

# What is Spark

A unified analytics engine for large-scale data processing.

* **Unified**: Spark supports many libraries, cluster technologies, and storage systems.
* **Analytics**:  Analytics is the discovery and interpretation of data to produce and communicate information.
* **Engine**: Spark is expected to be efficient and generic.
* **Large-Scale**: Cluster-scale using a set of connected computers working together.

![](intro-spark-unified-1.png)

---

# Dealing with Big Data or Big Compute

.large[
1. **Sampling**: reduce the amount of data by sampling. 
1. **Profiling**: Understand why a computation is slow and make the necessary improvements by profiling.
1. **Scaling Up**: Speeding up computation by using faster or more capable hardware.
1. **Scaling Out**: Spreading computation and storage across multiple machines.
]

---

# Sparklyr

.large[
`sparklyr` integrates with many R like `dplyr`, `magrittr`, `broom`, `DBI`, `tibble`, `rlang`, and many others
]
---

# Setup Local Cluster

## Setup java 8
```{r, echo=TRUE}
java8_home = system("/usr/libexec/java_home -v 1.8", intern = TRUE)
Sys.setenv(JAVA_HOME = java8_home)
system("java -version")
```

## Setup Spark and Sparklyr
```{r, echo=TRUE, message=FALSE}
if (!require(sparklyr)) {
  install.packages("sparklyr")
}
library(sparklyr)
spark_install("2.3")
```

---

# Connection

`sc` is the standard variable name to represent a handle to the spark cluster

```{r, echo=TRUE}
sc <- spark_connect(master = "local", version = "2.3")
```

What class is the `sc` object
```{r, echo=TRUE, comment=""}
class(sc)
```

---

# Move data from R to Spark

Copy mtcars to the cluster

```{r, echo=TRUE, comment=""}
cars <- copy_to(sc, mtcars)
cars
```
Row count is missing

---

# Classes

```{r, echo=TRUE, comment=""}
class(cars)
```

It inherits the tibble (`tbl`) class. Thus, tidyverse methods will generally work.

---

# Web Interface

```{r, echo=TRUE, eval=FALSE}
spark_web(sc)
```

![](starting-spark-web-resized.png)
---

# Analysis

## Using `DBI`

```{r, echo=TRUE, comment=""}
library(DBI)
dbGetQuery(sc, "SELECT COUNT(*) FROM mtcars")
```

## Using `dplyr`

```{r, echo=TRUE, comment=""}
library(dplyr)
count(cars)
```

---

# Chaining `dplyr` commands

```{r, echo=TRUE}
select(cars, hp, mpg) %>%
  sample_n(100) %>%
  collect() %>%
  plot()
```

---

# Modelling

Spark has a large library of modelling commands.

You cannot use the standard R models

```{r, echo=TRUE}
model <- ml_linear_regression(cars, mpg ~ hp)
model
```

---

# Serialization

Supports a wide collection of methods to serialize and deserialize data

## CSV

```{r}
tmp_dir = file.path("/tmp", "spark")
dir.create(tmp_dir, recursive = TRUE, showWarnings = FALSE)
file = file.path(tmp_dir, "cars.csv")
```

```{r, echo=TRUE}
spark_write_csv(cars, file)  # Serialize
cars_csv <- spark_read_csv(sc, file) # Deserialize
```

```{r}
unlink(tmp_dir, recursive = TRUE, force = TRUE)
```

### JSON
* `spark_read_json()`
* `spark_write_json()`

---

# Streaming

Write multiple files to a directory
```{r}
tmp_dir = file.path('/tmp', 'spark')
input_path = file.path(tmp_dir, "input")
dir.create(input_path, recursive = TRUE, showWarnings = FALSE)
```

```{r, echo=TRUE}
write.csv(mtcars, file.path(input_path, "cars_1.csv"), row.names = FALSE)
write.csv(mtcars, file.path(input_path, "cars_2.csv"), row.names = FALSE)
write.csv(mtcars, file.path(input_path, "cars_3.csv"), row.names = FALSE)
```

Stream each file in and write results to a directory
```{r}
output_path = file.path(tmp_dir, "output")
```
```{r, echo=TRUE}
stream <- stream_read_csv(sc, input_path) %>%
    select(mpg, cyl, disp) %>%
    stream_write_csv(output_path)
```

See the output files
```{r, echo=TRUE}
dir(output_path, pattern = ".csv")
```

```{r}
unlink(tmp_dir, recursive = TRUE, force = TRUE)
```

---

# Logs & Disconnect

## Logs

* Use the log button in the `Connections`
* Use `spark_log(sc)` or `spark_log(sc, filter = "INFO sparklyr: Gateway")`

## Disconnect

* `spark_disconnect(sc)`
* `spark_disconnect_all()`

---

# `dplyr` writes Spark SQL commands

```{r, echo=TRUE, warning=FALSE}
summarise_all(cars, mean) %>% 
  show_query()
```

If the function is not supported by `dpylr` it is passed onto Spark. Thus, you can use Spark functions.

---

# Visualizations

* Pass only aggregated data back to R from Spark 
* `dbplot` works with Spark and `ggplot` to do this
* `dbplot::dbplot_raster` will aggregate data for scatter plots
```{r, echo=TRUE, fig.height=4}
dbplot::dbplot_raster(cars, mpg, wt, resolution = 16)
```

---

# Caching

* Cache a result in Spark memory for future use
* use `compute()` with a name 
```{r, echo=TRUE}
cached_cars <- cars %>% 
  mutate(cyl = paste0('cyl_', cyl)) %>% 
  compute("cached_cars")
```
---

# Questions 1
1. Install Spark V2.3, and `sparklyr`
1. What version of Hadoop is installed?
1. Is Spark V3.0 available?
1. Make a connection to the local spark cluster.
1. Load the `zillow.csv` file into Spark.
1. Using the web interface, find the job that you just ran.
1. Using the web interface, find the objects that are loaded into memory.
1. Using RStudio UI, find the objects that are loaded into Spark.
1. Write a Spark SQL command to get the mean list price by Zip code.
1. Create a scatter plot of year built vs living space.
1. Create a linear regression on the list price using all the features except Zip and index.
1. Predict the list price of a home with 2000 sq ft, 3 Beds, 2 Baths, Year = 2000
1. You needed to create a data.frame in Spark to do the above command. How many bytes were used to create it?
1. Make three copies of the `zillow.csv` file (`zillow1.csv`, `zillow2.csv`, `zillow3.csv`) and scream read them into Spark, select the Zip and List Price, and write it out to `output/`
1. Programmatically check the logs for all the warning messages
1. Get the maximum value in each column of the `zillow` dataset.

---

# Questions 2
1. What is the Spark SQL used to create the above command.
1. Using the `stddev_samp` method in Spark. Compute the standard deviation of the sample for the list price.
1. Create a histogram of the list price. Make the bin width 100,000. Have Spark compute the actual histogram (i.e. due not pull the data back into R)
1. Compute the mean listing price by zip code and store it in Spark with the variable name `MEAN_BY_ZIP`.
1. What is the actual variable name in Spark for `MEAN_BY_ZIP`?
1. Disconnect R from Spark
